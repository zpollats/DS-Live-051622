{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, cross_validate, KFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, make_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- avoid letting information about test sets get into the training of models\n",
    "- use best practices for building non-leaky workflows\n",
    "- repair leaky workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have encountered the idea of splitting our data into two, *training* our model on one bit and then *testing* it on the other.\n",
    "\n",
    "The goal is to have an unbiased assessment of our model, and so we want to make sure that nothing about our test data sneaks into the training run of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Mistake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider the following workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_diabetes()\n",
    "\n",
    "print(data.DESCR)\n",
    "\n",
    "df = pd.concat([pd.DataFrame(data.data, columns=data.feature_names),\n",
    "               pd.Series(data.target, name='target')], axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_diabetes(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler().fit(X)\n",
    "X_scld = ss.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_scld, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "print(lr.coef_, lr.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>What did we do wrong</summary>\n",
    "Well we fit the model only to our training data. Looks like we've done everything right, right?\n",
    "\n",
    "It's important to understand that the answer here is a resounding \"NO\". It's true that we didn't directly fit our model to our test data. But the trouble is that we fit our scaler **to the whole dataset**. That is, records in the test data are contributing to calculations of column means and standard deviations, and so, surreptitiously, information about our test set is sneaking into the training run of the model after all!\n",
    "\n",
    "To correct our mistake, we'll make sure to perform our train-test split **first**:\n",
    " \n",
    "    \n",
    "<code>X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, random_state=42)\n",
    "    ss2 = StandardScaler().fit(X_train2)\n",
    "    X_train2_scld = ss2.transform(X_train2)\n",
    "    X_test2_scld = ss2.transform(X_test2)\n",
    "    lr2 = LinearRegression()\n",
    "    lr2.fit(X_train2_scld, y_train2)\n",
    "    print(lr2.coef_, lr2.intercept_)\n",
    "</code>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correct code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our model coefficients are slightly different from what they were before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Comparison\n",
    "\n",
    "It's worth pointing out that, **for linear models**, there is **no** difference in modeling error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_hat = lr.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_test_hat)\n",
    "print(f\"Our test RMSE for this model is {round(np.sqrt(mse), 2)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test2_hat = lr2.predict(X_test2_scld)\n",
    "mse = mean_squared_error(y_test2, y_test2_hat)\n",
    "print(f\"Our test RMSE for this model is {round(np.sqrt(mse), 2)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will **not** be true for other sorts of models that use different loss functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general all preprocessing steps are subject to the same dangers here. Consider the preprocessing step of one-hot-encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gun_poll = pd.read_csv('data/guns-polls.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gun_poll.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gun_poll['Pollster'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if I were to fit a one-hot encoder to the whole `Pollster` column here, the encoder would learn all the categories. But I need to prepare myself for the real-world possibility that unfamiliar categories may show up in future records. Let's explore this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First I'll do a split\n",
    "X_train, X_test = train_test_split(gun_poll, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's suppose now that I fit a `OneHotEncoder` to the `Pollster` column in my training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit an encoder to the `Pollster` column of the training data and then check to see which categories are represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Answer</summary>\n",
    "<code>to_be_dummied = X_train[['Pollster']]\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(to_be_dummied)\n",
    "## So what categories do we have?\n",
    "ohe.get_feature_names()</code>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll want to transform both train and test after we've fitted the encoder to the train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_to_be_dummied = X_test[['Pollster']]\n",
    "\n",
    "ohe.transform(to_be_dummied)\n",
    "ohe.transform(test_to_be_dummied)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are categories in the testing data that don't appear in the training data! What should \n",
    "we do about that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Strategy 1**: Divide up the categories proportionally when we do our train_test_split. If we're using `sklearn`'s tool, that means taking advantage of the `stratify` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement Strategy 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, in this case, we can't use this since some categories have only a single member."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Strategy 2**: Drop the categories with very few representatives.\n",
    "\n",
    "In the present case, let's try dropping the single-member categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement Strategy 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could now split this carefully so that new categories don't show up in the testing data. In fact, now we can try the stratified split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform a stratified split with those single-member categories dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train3['Pollster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test3['Pollster'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now every category that appears in the test data appears also in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Strategy 3**: Adjust the settings on the one-hot-encoder.\n",
    "\n",
    "For `sklearn`'s tool, we'll tweak the `handle_unknown` parameter:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a new encoder to our training data column that won't break when we try to use it to transform the test data. And then use the encoder to transform both train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Answer</summary>\n",
    "<code>ohe2 = OneHotEncoder(handle_unknown='ignore')\n",
    "ohe2.fit(to_be_dummied)\n",
    "test_to_be_dummied = X_test[['Pollster']]\n",
    "ohe2.transform(to_be_dummied)\n",
    "ohe2.transform(test_to_be_dummied)</code>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leakage into Validation Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we employ cross-validation, then our training data points will be serving both for training and for validation. So there's a sense in which we can't help but let some information about our validation data sneak into the model.\n",
    "\n",
    "But strictly speaking, cross-validation means building *multiple* models, and we still want each to be blind to its validation set.\n",
    "\n",
    "The dangers of data leakage, therefore, are still very much real in the case of validation data. And they are often more subtle as well. Consider the following line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = make_scorer(mean_squared_error, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using our scaled training data\n",
    "\n",
    "cv_results = cross_validate(estimator=LinearRegression(),\n",
    "                X=X_train2_scld,\n",
    "                y=y_train2,\n",
    "                scoring=mse,\n",
    "                return_estimator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at model coefficients on the first predictor\n",
    "\n",
    "[model.coef_[0] for model in cv_results['estimator']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>No leaks right?</summary>\n",
    "We've built five models here, and none of them saw any points from the test data, so we have no leaks, right?\n",
    "\n",
    "Wrong! We fit the `StandardScaler` to the whole training set, which means that information about *every* fold will affect every cross-validation. A better practice here would be to split our data into its cross-validation folds *first*. Then we can fit the scaler to only the training folds for each cross-validation.\n",
    "\n",
    "Of course, the more preprocessing steps we have, the more tedious it becomes to do this work! For such tasks it is often greatly beneficial to take advantage of `sklearn`'s `Pipeline`s, which we'll have more to say about later.\n",
    "\n",
    "For now, let's see if we can fix our leaky cross-validation scorer.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go back to `X_train2` and do the following:\n",
    "\n",
    "- Split it into five validation folds (Use `KFold()`).\n",
    "- For each split:\n",
    "- (i) fit a `StandardScaler` to the four-fold chunk and transform all data points with it.\n",
    "- (ii) fit a `LinearRegression` to the four-fold chunk and print out the value of the first coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Answer</summary>\n",
    "\n",
    "<code>for train_ind, val_ind in KFold().split(X_train2):\n",
    "    train = X_train2[train_ind, :]\n",
    "    val = X_train2[val_ind, :]\n",
    "    target_train = y_train2[train_ind]\n",
    "    target_val = y_train2[val_ind]\n",
    "    ss = StandardScaler().fit(train)\n",
    "    train_scld = ss.transform(train)\n",
    "    val_scld = ss.transform(val)\n",
    "    lr = LinearRegression().fit(train_scld, target_train)\n",
    "    print(lr.coef_[0])</code>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Contrived But Illustrative Example\n",
    "\n",
    "This is only a slight difference in coefficients, but for other datasets the difference can be great."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_preds = np.array([6, 5, 2, 250, 300]).reshape(-1, 1)\n",
    "fake_target = np.array([25, 30, 12, 400, 420]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.hstack((fake_preds, fake_target)),\n",
    "                 columns=['pred', 'target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train = df[['pred']][:3]\n",
    "small_val = df[['pred']][3:]\n",
    "small_train_y = df['target'][:3]\n",
    "small_val_y = df['target'][3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the whole dataset\n",
    "\n",
    "ss = StandardScaler().fit(df[['pred']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = ss.transform(small_train)\n",
    "X_va = ss.transform(small_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression().fit(X_tr, small_train_y)\n",
    "lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "X = np.linspace(0, 310, 600)\n",
    "y = lr.coef_ * X + lr.intercept_\n",
    "\n",
    "ax.scatter(df['pred'], df['target'])\n",
    "ax.plot(X, y)\n",
    "ax.set_title(f\"\"\"The best-fit line when scaling the whole dataset is\n",
    "    {round(lr.coef_[0])}X + {round(lr.intercept_)}\"\"\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting before scaling\n",
    "\n",
    "ss2 = StandardScaler().fit(small_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr2 = ss2.transform(small_train)\n",
    "X_va2 = ss2.transform(small_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr2 = LinearRegression().fit(X_tr2, small_train_y)\n",
    "lr2.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "X = np.linspace(0, 310, 600)\n",
    "y = lr2.coef_ * X + lr2.intercept_\n",
    "\n",
    "ax.scatter(df['pred'], df['target'])\n",
    "ax.plot(X, y)\n",
    "ax.set_title(f\"\"\"The best-fit line when scaling after splitting is\n",
    "    {round(lr2.coef_[0])}X + {round(lr2.intercept_)}\"\"\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
