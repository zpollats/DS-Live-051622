{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra Applications in Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Source](https://www.analyticsvidhya.com/blog/2019/07/10-applications-linear-algebra-data-science/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from seaborn import load_dataset\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "gems = load_dataset('diamonds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectors and Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of linear algebra, a single number is a 0-dimensional entity called a **scalar**. But it is often useful to have data in the form of a 1-dimensional object called a **vector**, which can be thought of as a list of scalars. Think here of a `pandas` Series. And in addition to the values that compose the vector, we can characterize the vector as a whole as having a **magnitude** and a **direction**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot([0, 3], [0, 4], 'b')\n",
    "ax.vlines(1, ymin=0, ymax=4/3)\n",
    "ax.scatter(0, 0, s=30)\n",
    "\n",
    "# arrowhead!\n",
    "ax.vlines(3, ymin=3.7, ymax=4, colors='b')\n",
    "ax.hlines(4, xmin=2.8, xmax=3, colors='b')\n",
    "ax.annotate('$\\phi$', xy=(1.1, 0.5))\n",
    "ax.annotate('|v| = 5', xy=(0.9, 2.3))\n",
    "ax.set_xlim(right=5)\n",
    "ax.set_ylim(top=6);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have been working all along with arrays and data frames that have a tabular structure of rows and columns. Such a 2-dimensional structure of numerical elements is known in linear algebra as a **matrix**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gems.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gems.head().values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we want 3- or even higher-dimensional objects. Think for example of a digital image where we record the red, green, and blue values *for each pixel in the 2d array*. The linear algebraic abstraction we need for such an object is called a **tensor**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42) #setting a random seed\n",
    "\n",
    "#generate random values for a numpy array of shape (3,5,5), and round those values to 1 digit\n",
    "tensor = np.round(np.random.rand(3, 5, 5), 1)\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor[0][0][0]  #access the (0,0,0)th element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Arithmetic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrices can be added and multiplied, and there are other distinctive operations on matrices that are often useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>Matrix Addition</b>: Click for Illustration</summary>\n",
    "$\\begin{bmatrix}\n",
    "a_{11} & a_{12} \\\\\n",
    "a_{21} & a_{22}\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "b_{11} & b_{12} \\\\\n",
    "b_{21} & b_{22}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "a_{11} + b_{11} & a_{12} + b_{12} \\\\\n",
    "a_{21} + b_{21} & a_{22} + b_{22}\n",
    "\\end{bmatrix}\n",
    "$\n",
    "    </details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "#return a numpy array of shape (2,2) with random integers from 1 inclusive to 11 exclusive\n",
    "my_matrix1 = np.random.randint(low=1, high=11, size=(2, 2))\n",
    "my_matrix1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_matrix2 = np.random.randint(low=1, high=11, size=(2, 2))\n",
    "my_matrix2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform matrix addition with our two matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>Matrix Multiplication</b>: Click for Illustration</summary>\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "a_{1,1} & a_{1,2} \\\\\n",
    "a_{2,1} & a_{2,2}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "b_{1,1} & b_{1,2} \\\\\n",
    "b_{2,1} & b_{2,2}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "a_{1,1}\\times b_{1,1} + a_{1,2}\\times b_{2,1} & a_{1,1}\\times b_{1,2} + a_{1,2}\\times b_{2,2} \\\\\n",
    "a_{2,1}\\times b_{1,1} + a_{2,2}\\times b_{2,1} & a_{2,1}\\times b_{1,2} + a_{2,2}\\times b_{2,2}\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "$$\n",
    "    </details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform matrix multiplication with our two matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many times when we need to measure distances. For example, many modeling algorithms rely on a notion of **similarity** between data points. But we have already seen how distance is used to construct a linear model: Choose the betas that minimize the sum of squared **distances** between true and predicted $y$-values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider this distance for *all* data points at once: We can think of that as a vector: $\\vec{(y_i - \\hat{y_i})^2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And in fact there are multiple ways to measure the magnitude of a vector. Typically, we are thinking of Euclidean spaces and so use the **L2 norm** to measure the magnitude of a vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_vec = np.array([3, 4, 12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(3**2 + 4**2 + 12**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use numpy method to get magnitude of a vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But there are other norms we can use. In general, the **$n$-norm** will calculate $(x_1^n + ... + x_m^n)^{\\frac{1}{n}}$ for a vector $\\vec{x_i}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use numpy method to get the n-norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can express covariance and correlation matrices as linear-algebraic transformations:\n",
    "\n",
    "For a centered data matrix $M$:\n",
    "- $cov(M) = \\frac{1}{n-1}M^TM$, where $n$ is the number of observations.\n",
    "\n",
    "A centered data matrix is one whose column means are all 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This equation makes use of the **transpose** of a matrix $M$, $M^T$, which is the matrix that results from swapping the rows and columns of $M$. You can also think of this as a *reflection* of the elements of $M$ about the main diagonal of $M$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_matrix1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the transpose of my matrix\n",
    "my_matrix_transposed = \n",
    "my_matrix_transposed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's illustrate this equation. Suppose we have ten observations (rows) for each of three variables (columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "mat_1 = np.random.rand(10, 3)\n",
    "mat_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_1_centered = mat_1 - np.mean(mat_1, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_1_centered.T.dot(mat_1_centered) / 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use numnpy to obtain the covariance matrix\n",
    "cov = \n",
    "cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the correlation matrix by hand\n",
    "stds = np.sqrt(np.diag(cov))\n",
    "np.diag(stds**-1).dot(cov).dot(np.diag(stds**-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use numpy to obtain the correlation matrix\n",
    "corr = \n",
    "corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that it is difficult for us to interpret covariances because covariance values are very dependent on the specific data values, hence why we look at **correlation, which gives us the covariance on a standardized scale**. If we are looking at the covariance between variables $X_1$ and $X_2$, the correlation is simply standardizing the covariance by the standard deviations of $X_1$ and $X_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Bonus: Correlation Matrices</summary>\n",
    "    To calculate a correlation matrix, we can multiply the covariance matrix on both sides by a diagonal matrix of the reciprocals of the standard deviations of the columns. [Source](https://blogs.sas.com/content/iml/2010/12/10/converting-between-correlation-and-covariance-matrices.html)\n",
    "\n",
    "<code>stds = np.sqrt(np.diag(cov))\n",
    "np.diag(stds\\*\\*-1).dot(cov).dot(np.diag(stds\\*\\*-1))\n",
    "np.corrcoef(mat_1, rowvar=False)\n",
    "</code>\n",
    "    </details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving a System of Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In elementary algebra we start by solving one equation for one unknown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image from mathelp.org](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSnrLW6J0FYge6zWDKqRrAtWx4Jf0HkhMiwHQ&usqp=CAU) source: mathelp.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear algebra gives us the tools to solve many equations simultaneously. Suppose we have:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\begin{align}\n",
    " x_1 - 2x_2 + 3x_3 &= 9 \\\\\n",
    " 2x_1 - 5x_2 + 10x_3 &= 4 \\\\\n",
    " 6x_3 &= 0 \n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write these equations as a single matrix equation:\n",
    "\n",
    "$$ \n",
    "\\begin{bmatrix} \n",
    "    1 & -2 & 3 \\\\\n",
    "    2 & -5 & 10 \\\\\n",
    "    0 & 0 & 6\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} \n",
    "    x_1 \\\\\n",
    "    x_2 \\\\\n",
    "    x_3\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} \n",
    "    9 \\\\\n",
    "    4 \\\\\n",
    "    0\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or: $A\\vec{x} = \\vec{b}$, where\n",
    "\n",
    "- $A = \\begin{bmatrix} \n",
    "    1 & -2 & 3 \\\\\n",
    "    2 & -5 & 10 \\\\\n",
    "    0 & 0 & 6\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} \n",
    "    x_1 \\\\\n",
    "    x_2 \\\\\n",
    "    x_3\n",
    "\\end{bmatrix}$\n",
    "\n",
    "- $\\vec{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}$, and\n",
    "\n",
    "- $\\vec{b} = \\begin{bmatrix} 9 \\\\ 4 \\\\ 0 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we're solving for a *vector* of unknowns. To solve $A\\vec{x} = \\vec{b}$ for $x$, we multiply both sides of the equation by **$A^{-1}$, the inverse of $A$**:\n",
    "\n",
    "$A^{-1}A\\vec{x} = \\vec{x} = A^{-1}b$\n",
    "\n",
    "In just the way that multiplying a scalar by its multiplicative inverse produces 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "42 * 42**-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so multiplying a matrix by its matrix inverse produces $I$, the **identity matrix**, a square matrix with 1's down the main diagonal and 0's everywhere else.\n",
    "\n",
    "> $$\\begin{align}\n",
    "    I_3 &= \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\\\\n",
    "    \\\\\n",
    "    I_5 &= \\begin{bmatrix} 1 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "                           0 & 1 & 0 & 0 & 0 & 0 \\\\\n",
    "                           0 & 0 & 1 & 0 & 0 & 0 \\\\\n",
    "                           0 & 0 & 0 & 1 & 0 & 0 \\\\\n",
    "                           0 & 0 & 0 & 0 & 1 & 0 \\\\\n",
    "                           0 & 0 & 0 & 0 & 0 & 1 \\\\                           \n",
    "            \\end{bmatrix}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inverse and identity matrices have important properties:\n",
    "\n",
    "- $IA = A$\n",
    "- $AI = A$\n",
    "- $AA^{-1} = I$\n",
    "- $A^{-1}A = I$\n",
    "- $I\\vec{x} = \\vec{x}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([\n",
    "    [1, -2,  3],\n",
    "    [2, -5, 10],\n",
    "    [0,  0,  6]\n",
    "])\n",
    "\n",
    "b = np.array([9, 4, 0]).reshape(3, 1)\n",
    "\n",
    "print('A:')\n",
    "print(A)\n",
    "print()\n",
    "print('b:')\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the inverse\n",
    "\n",
    "A_inv =\n",
    "print(A_inv)\n",
    "\n",
    "# Getting the solution\n",
    "\n",
    "x1, x2, x3 = \n",
    "print(f\"x1 = {x1[0]}, x2 = {x2[0]}, x3 = {x3[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.dot(np.array([x1, x2, x3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve It Faster with NumPy's `linalg.solve()`\n",
    "\n",
    "NumPy's ```linalg``` module has a ```.solve()``` method that you can use to solve a system of linear equations!\n",
    "\n",
    "In particular, it will solve for the vector $\\vec{x}$ in the equation $A\\vec{x} = b$. You should know that, \"under the hood\", the ```.solve()``` method does NOT compute the inverse matrix $A^{-1}$. This is largely because of the enormous expense of directly computing a matrix inverse, which takes $\\mathcal{O}(n^3)$ time.\n",
    "\n",
    "Check out [this discussion](https://stackoverflow.com/questions/31256252/why-does-numpy-linalg-solve-offer-more-precise-matrix-inversions-than-numpy-li) on stackoverflow for more on the differences between using `.solve()` and `.inv()`.\n",
    "\n",
    "And check out the documentation for ```.solve()``` [here](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.solve.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the .solve() method to solve this system of equations\n",
    "\n",
    "A = np.array([\n",
    "    [1, -2,  3],\n",
    "    [2, -5, 10],\n",
    "    [0,  0,  6]\n",
    "])\n",
    "\n",
    "b = np.array([9, 4, 0]).reshape(3, 1)\n",
    "\n",
    "#Use the solve method to solve this system of equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we could just solve our matrix equation by calculating the inverse of our matrix $A$ and then multiplying by $b$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.inv(A).dot(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the time difference is striking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit np.linalg.inv(A).dot(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit np.linalg.solve(A, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even for a (tiny!) 5x5 matrix, the cost of computing the inverse directly is evident."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider now a typical dataset and the associated multiple linear regression problem. We have many observations (rows), each of which consists of a set of values both for the predictors (columns, i.e. the independent variables) and for the target (the dependent variable).\n",
    "\n",
    "For the equation $A\\vec{x} = \\vec{c}$, we can think of the values of the independent variables (i.e. the data matrix, \"X\") as our matrix $A$ of coefficients and of the values of the dependent variable (i.e. the target, \"y\") as our output vector $\\vec{c}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task here is, in effect, to solve for $\\vec{\\beta}$, where we have that $A\\vec{\\beta} = \\vec{c}$, except in general we'll have more rows than columns. But more rows than columns means more equations than unknowns, which means that in general **there is no solution**. This is why instead we go for an optimization--in our case, a best-fit line. So we have $A\\vec{\\beta}\\approx\\vec{c}$.\n",
    "\n",
    "Using $a$ for our independent variables and $c$ for our dependent variable, we have:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\beta_1\\begin{bmatrix}\n",
    "a_{1,1} \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    "a_{m,1}\n",
    "\\end{bmatrix} +\n",
    "... + \\beta_n\\begin{bmatrix}\n",
    "a_{1,n} \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    "a_{m,n}\n",
    "\\end{bmatrix} \\approx \\begin{bmatrix}\n",
    "c_1 \\\\\n",
    ".  \\\\\n",
    ".  \\\\\n",
    ".  \\\\\n",
    "c_m\n",
    "\\end{bmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Algebra Solves the Best-Fit Line Problem\n",
    "\n",
    "If we have a matrix of predictors $X$ and a target column $y$, we can express $\\vec{\\beta}$, the vectorized parameters of the best-fit line, as  follows:\n",
    "\n",
    "$\\large\\vec{\\beta} = (X^TX)^{-1}X^Ty$.\n",
    "\n",
    "$(X^TX)^{-1}X^T$ is sometimes called the *pseudo-inverse* of $X$.\n",
    "\n",
    "Let's see this in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "preds = np.array(list(zip(np.random.normal(size=10),\n",
    "                          np.array(np.random.normal(size=10, loc=2)))))\n",
    "target = np.array(np.random.exponential(size=10))\n",
    "\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.inv(preds.T.dot(preds)).dot(preds.T).dot(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinearRegression(fit_intercept=False).fit(preds, target).coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Taste of What's Coming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvalues, Singular Values, Principal Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often useful to express a matrix as a **product** of other matrices. Sometimes the gain is only in computational efficiency, but there are also certain factorizations or **decompositions** that are useful in other ways.\n",
    "\n",
    "An **eigendecomposition** reduces a matrix to a collection of vectors that capture the *linear* action of the matrix. Selecting the vectors that produce the largest such linear transformations is the idea behind **principal component analysis**, which is useful for reducing high-dimensional datasets to lower-dimensional problems.\n",
    "\n",
    "Eigendecompositions are possibly only for square matrices; a **singular value decomposition** is a more fundamental matrix factorization that can be applied to any matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendation Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do recommendation engines work?\n",
    "\n",
    "Imagine representing your interests (film genres, book subjects, music styles) as a **vector**: larger numbers represent larger preferences. Now do this for multiple people. Now we can think of comparing these vectors directly or against some target such as whether a given product/service was used/bought/watched."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When our data is **unlabeled** we have a problem in **unsupervised learning**. One major strategy for this type of problem is to impose a *similarity* metric on our data points. Similarity between data points is measured as some function of the **(vector) distance** between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One similarity metric for vectors is **cosine similarity**, which computes the *cosine of the angle between them*. Note that this is always well-defined for non-zero vectors since any two vectors determine a plane (in which the angle can be measured)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Images\n",
    "\n",
    "We saw already above the idea of representing a digital image as a **tensor** of values that encode facts about each pixel in the digitization.\n",
    "\n",
    "**Neural networks** are good for working with tensors of high dimension. Such objects often need to be manipulated into different shapes, and the `.reshape()` method is great for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor.reshape(5, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor.reshape(1, 75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level Up: Matrix Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many transformations of *products* of matrices can be expressed in terms of the transformation applied to the factors *in reverse order*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(AB)^T = B^TA^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.randint(low=1, high=11, size=(10, 2))\n",
    "B = np.random.randint(low=1, high=11, size=(2, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(A.dot(B)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B.T.dot(A.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(AB)^{-1} = B^{-1}A^{-1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.randint(low=1, high=11, size=(3, 3))\n",
    "B = np.random.randint(low=1, high=11, size=(3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.inv(A.dot(B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.linalg.inv(B).dot(np.linalg.inv(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level Up: The Determinant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determinant\n",
    "\n",
    "The **determinant** of a square matrix $M$, $|M|$, represents the area (or, in higher dimensions, the volume) of the parallelogram (parallelepiped) formed by the rows or columns of $M$. And it is also related to the inverse of $M$.\n",
    "\n",
    "For a 2x2 matrix $\\begin{bmatrix} a & b \\\\ c & d\\end{bmatrix}$, the determinant is equal to $ad - bc$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_matrix1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.det(my_matrix1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = my_matrix1[0][0]\n",
    "d = my_matrix1[1][1]\n",
    "b = my_matrix1[0][1]\n",
    "c = my_matrix1[1][0]\n",
    "\n",
    "a*d - b*c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
